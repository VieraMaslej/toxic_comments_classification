# Toxic comments classification

## The impact of text pre-processing methods on the performance of deep learning models for the toxic comments classification

Viera Maslej Kresnakova, Martin Sarnovsky, Peter Butka, Kristina Machova

## Bibtex
@article{maslej2020comparison,
  title={Comparison of Deep Learning Models and Various Text Pre-Processing Techniques for the Toxic Comments Classification},
  author={Maslej-Kre{\v{s}}{\v{n}}{\'a}kov{\'a}, Viera and Sarnovsk{\`y}, Martin and Butka, Peter and Machov{\'a}, Krist{\'\i}na},
  journal={Applied Sciences},
  volume={10},
  number={23},
  pages={8631},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

### Abstract
The emergence of anti-social behaviour in online environments presents a serious issue in todayâ€™s society. Automatic detection and identification of such behaviour are becoming increasingly important. Modern machine learning and natural language processing methods can provide effective tools to detect different types of anti-social behaviour from the pieces of text. In this work, we present a comparison of various deep learning models used to identify the toxic comments in the Internet discussions. Our main goal was to explore the effect of the data preparation on the model performance. As we worked with the assumption that the use of traditional pre-processing methods may lead to the loss of characteristic traits, specific for toxic content, we compared several popular deep learning and transformer language models. We aimed to analyze the influence of different pre-processing techniques and text representations including standard TF-IDF, pre-trained word embeddings and also explored currently popular transformer models. Experiments were performed on the dataset from the Kaggle Toxic Comment Classification competition, and the best performing model was compared with the similar approaches using standard metrics used in data analysis

#### Dataset
Dataset available on https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data

#### Embeddings
Glove pre-trained embeddings: https://nlp.stanford.edu/projects/glove/ <br>
FastText pre-trained embeddings: https://fasttext.cc/ <br>
Gensim Word2vec: https://radimrehurek.com/gensim/models/word2vec.html <br>
Transformers - Models and tokenizers : https://huggingface.co/transformers/ <br>


